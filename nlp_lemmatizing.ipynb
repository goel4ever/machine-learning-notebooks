{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP69OBaCTtXhaDXU0hFw34Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goel4ever/machine-learning-notebooks/blob/main/nlp_lemmatizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP: Lemmatizing\n",
        "\n",
        "A `lemma` is a word that represents a whole group of words, and that group of words is called a `lexeme`.\n",
        "\n",
        "Like stemming, `lemmatizing` reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word like \"discoveri\". This notebook focuses on reducing words to their core meaning using Natural Language Processing.\n",
        "\n",
        "We'll use NLTK package for implementation. A group of texts is called a corpus. NLTK provides several corpora covering everything from novels hosted by Project Gutenberg to inaugural speeches by presidents of the United States.\n",
        "\n",
        "In order to analyze texts in NLTK, you first need to import them. We need a one-off run of nltk.download() to get all the resources in one go. Note: It will take some time."
      ],
      "metadata": {
        "id": "MT2-zJyw7Jxz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXGlsaiC7BdT",
        "outputId": "98de21c3-e1d9-4b36-c3df-8b982a3ac677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# Download resource wordnet for lemmatization\n",
        "nltk.download('wordnet')\n",
        "# Download resource wordnet for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Required imports\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "R-3G7l5E8ENt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test lemmatizing a plural noun\n",
        "lemmatizer.lemmatize(\"scarves\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l77_Yi4X8LKq",
        "outputId": "0b2c741c-9151-4a3b-affb-46a34e4f0e64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'scarf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize a string\n",
        "string_for_lemmatizing = \"The friends of DeSoto love scarves.\"\n",
        "words = word_tokenize(string_for_lemmatizing)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne3_YYaR8pwl",
        "outputId": "40872d6f-62f6-406f-8ebd-75103c6b23f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'friends', 'of', 'DeSoto', 'love', 'scarves', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize the words\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "lemmatized_words\n",
        "# Note the plurals 'friends' and 'scarves' became the singulars 'friend' and 'scarf'."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6I6drAa9KJe",
        "outputId": "549c41cf-3e31-4031-f83d-697e447103e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario where you lemmatized a word that looked very different from its lemma"
      ],
      "metadata": {
        "id": "JQnjZC_Q9sgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You'll get the result 'worst' because lemmatizer.lemmatize() assumes that \"worst\" was a noun.\n",
        "lemmatizer.lemmatize(\"worst\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZSHt6o1o9nQf",
        "outputId": "f2fadcdd-e558-42aa-fae3-deb5b14a43e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can make it clear that you want \"worst\" to be an adjective\n",
        "lemmatizer.lemmatize(\"worst\", pos=\"a\")\n",
        "# The default parameter for pos is 'n' for noun, but you made sure that \"worst\" was treated as an adjective"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gbSCgC7H-Dzp",
        "outputId": "bc533ab5-98f1-4fa0-a899-fe6678f13102"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}