{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGd6JYs7l785kR6aKD4tBd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goel4ever/machine-learning-notebooks/blob/main/nlp_chinking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP: Chinking\n",
        "\n",
        "`Chinking` is used together with `chunking`, but while chunking is used to include a pattern, chinking is used to `exclude a pattern`."
      ],
      "metadata": {
        "id": "fh0exD3lcXFi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeFLCLWlcTmf",
        "outputId": "053e3bdd-db0a-4280-9303-6836a5770f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# Download resource punkt for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Required imports\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before you can chunk, you need to make sure that the parts of speech in your text are tagged, so create a string for POS tagging.\n",
        "# We'll reuse the quote used in chunking\n",
        "quote = \"It's a dangerous business, Frodo, going out your door.\""
      ],
      "metadata": {
        "id": "s86V-cuHcnMg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the string by word\n",
        "words_in_quote = word_tokenize(quote)\n",
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8P7Xhcccqk6",
        "outputId": "475b8997-e0a2-4c42-eabe-a218badc7021"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It',\n",
              " \"'s\",\n",
              " 'a',\n",
              " 'dangerous',\n",
              " 'business',\n",
              " ',',\n",
              " 'Frodo',\n",
              " ',',\n",
              " 'going',\n",
              " 'out',\n",
              " 'your',\n",
              " 'door',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tag those words by part of speech\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "pos_tags = nltk.pos_tag(words_in_quote)\n",
        "pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kfwb8NDFcu2x",
        "outputId": "a4749f54-5ef3-45cc-b039-ce23b65d19bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It', 'PRP'),\n",
              " (\"'s\", 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('dangerous', 'JJ'),\n",
              " ('business', 'NN'),\n",
              " (',', ','),\n",
              " ('Frodo', 'NNP'),\n",
              " (',', ','),\n",
              " ('going', 'VBG'),\n",
              " ('out', 'RP'),\n",
              " ('your', 'PRP$'),\n",
              " ('door', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a grammar to determine what we want to include and exclude in our chunks\n",
        "grammar = \"\"\"\n",
        "Chunk: {<.*>+}\n",
        "        }<JJ>{\n",
        "\"\"\"\n",
        "\n",
        "# Because we’re using more than one line for the grammar, we’ll be using triple quotes (\"\"\")\n",
        "# The first rule is with curly braces because it’s used to determine what patterns you want to include in you chunks.\n",
        "# The second rule is with curly braces reversed because it’s used to determine what patterns you want to exclude in your chunks."
      ],
      "metadata": {
        "id": "dqZHNiSUc5lc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chunk parser with the grammer\n",
        "chunk_parser = nltk.RegexpParser(grammar)"
      ],
      "metadata": {
        "id": "M-emOk5UeaNZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk the sentence with the chink\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# This will cause an error in notebooks because there's no display to draw the tree on\n",
        "# tree.draw()\n",
        "print(tree.pretty_print())\n",
        "\n",
        "# In this case, ('dangerous', 'JJ') was excluded from the chunks because it’s an adjective (JJ).\n",
        "# The first chunk has all the text that appeared before the adjective that was excluded.\n",
        "# The second chunk contains everything after the adjective that was excluded."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx5FmPIueg1u",
        "outputId": "fd3e5f4a-e736-4339-cdb3-4ceb97fa0f0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    S                                               \n",
            "      ______________________________________________|_____________                                   \n",
            "     |              Chunk                                       Chunk                               \n",
            "     |          ______|_____          ____________________________|_______________________________   \n",
            "dangerous/JJ It/PRP 's/VBZ a/DT business/NN ,/, Frodo/NNP ,/, going/VBG out/RP your/PRP$ door/NN ./.\n",
            "\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}